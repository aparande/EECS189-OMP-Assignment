{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7BDt0Ih0zmTV"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "from sklearn.datasets import load_boston\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOEFOEyDB4cG"
   },
   "source": [
    "# Orthogonal Matching Pursuit for Outlier Removal on a 1D Data Set\n",
    "\n",
    "  The first portion of this assignemnts introduces a basic example of using OMP for outlier removal using a simple linear data set. Using the provided functions, you will visualize a dataset with outliers and answer the following questions. The second task is to code the OMP Greedy Algorithm from scratch using only numpy functions and native python functions. This will then be used for the rest of the assignment so be sure that your algorithm works for the dummy dataset.\n",
    "  \n",
    "Recall that the pseudocode for OMP is provided for you in the note/lecture slides as well as the mathematical intuition behind the algorithm. Your goal here is to implement this algorithm on a dummy dataset for easy visualization and testing, and then to a real multidimensional data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELkOf6AfaX8D"
   },
   "source": [
    "# Outlier Detection on Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXATvsgeJX-E"
   },
   "outputs": [],
   "source": [
    "# Provided function for generating a dummy set \n",
    "def generate_data(n, d, num_outliers): # Code from HW2 Prob 5 OMP Outlier Removal for CS 189\n",
    "    \"\"\"\n",
    "    Generates the data for our problem.\n",
    "    \n",
    "    args:\n",
    "      n: number of samples\n",
    "      d: dimension of samples\n",
    "      num_outliers: how many outliers are in the noisy observations;\n",
    "        we model outliers by adding large Gaussian noise\n",
    "                      \n",
    "    returns:\n",
    "      data: the nxd data matrix X\n",
    "      w_star: the underlying dx1 matrix W\n",
    "      observations: nx1 vector of noisy observations with outliers\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    w_star = np.random.uniform(2.0,20.0,(d,1))\n",
    "    data = np.random.uniform(0.0,10.0,(n,d))\n",
    "    y_true = np.dot(data,w_star)\n",
    "    ind = np.random.choice(np.arange(n), num_outliers, False)\n",
    "    vec = 0.1*np.ones((n,1))\n",
    "    for a in ind:\n",
    "        vec[a] = 100.000\n",
    "    observations = np.random.multivariate_normal(\n",
    "                        np.ndarray.flatten(y_true), np.diagflat(vec))\n",
    "    for a in ind:\n",
    "        observations[a] += 40.000\n",
    "    return data, w_star, observations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XrFk064jD3wJ"
   },
   "source": [
    "## Visualize dummy data\n",
    "Using the function above that is taken from CS189 HW2 Q5: \n",
    "1. Generate a dataset of 50 points that is one dimensional so that you can visualize the graph. Then you can choose the inital number of outliers to anything between 1-20. \n",
    "2. Then using the sklearn library imported above split the data into a 80 percent size training set and a 20 percent size testing set. The students may choose whether to use the built in sklearn function or select the data themselves. \n",
    "3. Visualize the scatter plot of the training data\n",
    "4. Train a Linear Regression model on the training data and plot the learned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eL_OEcmZKOcl"
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "d = 1\n",
    "num_outliers = # Number of outliers\n",
    "X, wstar, y = # Get data\n",
    "X_train, X_test, y_train, y_test = # Split training and test\n",
    "\n",
    "# TODO: Plot X_train vs y_train in a scatter plot, train a model on this data and plot its predicted line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POyM45phE4JP"
   },
   "source": [
    "## Implementing OMP Algorithm\n",
    "In your plot, you should see that your learned function doesn't fit your true linear function exactly - it is being skewed by the outliers. In the lecture and note, we learned how OMP can be used for outlier detection. To help find a better regression line, we will utilize OMP on the dummy dataset.\n",
    "\n",
    "**Implement the OMP algorithm below.** The inputs and outputs have already been defined for you so all you have to code is the algorithm itself. For this part, **DO NOT USE: sklearn's OMP algorithm**. Only use numpy and native python functions to code the entirety of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkNCwuBHCg0b"
   },
   "outputs": [],
   "source": [
    "def omp_outlier_detection(X, y, k, threshold=0):\n",
    "    # Input:\n",
    "    #   X: Features of data to run outlier detection on\n",
    "    #   y: Labels of data to run outlier detection on\n",
    "    #   k: Sparsity (How many outliers to detect)\n",
    "    # Output:\n",
    "    #   F: Array of indices of outliers\n",
    "\n",
    "    # TODO: Implement OMP\n",
    "    \n",
    "    ##Initialize variables\n",
    "    #r = \n",
    "    \n",
    "    #while not converged\n",
    "    \n",
    "    ##find indicies\n",
    "    \n",
    "    #return indicies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCBaKEJKFbXs"
   },
   "source": [
    "Below we have provided the necessary code to interactively plot your data and show the outlier removal process as the sparisty value or k is changed in the process. The graphs that are produced will have associated questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T88QKAv8FhGG"
   },
   "outputs": [],
   "source": [
    "def generate_num_outliers():\n",
    "    return widgets.IntSlider(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=50,\n",
    "        step=1,\n",
    "        description='Sparisty k',\n",
    "        continuous_update=False)\n",
    "\n",
    "def visualize_OMP(k):\n",
    "  F = omp_outlier_detection(X_train, y_train, k)\n",
    "  outlierArray = X_train[F]\n",
    "  outlierValues = y_train[F]\n",
    "  plt.figure(1)\n",
    "  data1 = plt.scatter(X_train, y_train, color = 'blue')\n",
    "  data2 = plt.scatter(outlierArray, outlierValues, color = 'red')\n",
    "  plt.legend((data1, data2), ('True Function', 'Outliers'))\n",
    "  plt.xlabel(\"X Data\")\n",
    "  plt.ylabel(\"Y Data\")\n",
    "  plt.title(\"Graph before OMP\")\n",
    "\n",
    "  model = LinearRegression()\n",
    "  model.fit(X_train, y_train)\n",
    "  y_pred = model.predict(X_train)\n",
    "  plt.plot(X_train, y_pred, color='orange')\n",
    "\n",
    "  ## Second Plot without the outliers showing\n",
    "  plt.figure(2)\n",
    "  removedX = np.delete(X_train, F).reshape(-1, 1)\n",
    "  removedY = np.delete(y_train, F)\n",
    "  data3 = plt.scatter(removedX, removedY, color = 'green')\n",
    "\n",
    "  omp_model = LinearRegression()\n",
    "  omp_model.fit(removedX, removedY)\n",
    "  y_pred = omp_model.predict(removedX)\n",
    "  plt.plot(removedX, y_pred, color='orange')\n",
    "\n",
    "  plt.title(\"Graph after OMP\")\n",
    "  plt.xlabel(\"X Data\")\n",
    "  plt.ylabel(\"Y Data\")\n",
    "  #plt.legend((data3), ('Resultant Function'))\n",
    "\n",
    "interactive_plot = interactive(visualize_OMP,\n",
    "                               k=generate_num_outliers())\n",
    "interactive_plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lReIObeXFslF"
   },
   "source": [
    "From the Graphs Above answer the following question: \n",
    "1. **What occurs when the number of k outliers are removed? What happens if this is too high or too low in comparison to the number of actual outliers present?** <br>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "ANSWER: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lD2vi4kbaRE6"
   },
   "source": [
    "# Outlier Detection on the Forest Fires Dataset\n",
    "\n",
    "Now we will attempt to use OMP outlier detection on a real dataset; we will be working with Forest Fire Dataset sourced from UCI's machine learning data sets. The nature of this dataset is as follows:\n",
    "\n",
    ">There are 517 different data points in this data set that contribute to our X. Each of these datapoints contain 13 features or attributes: The x,y position of the fire in the park, month, day, FFMC, DMC, DC, ISI, temperature, Relative Humidity, Wind, Rain, and our output parameter as the area burned by the fire.\n",
    "\n",
    "We have imported the dataset below for you using the pandas dataframe. You must preprocess the data in the following way.\n",
    "# Preprocessing\n",
    "1. Cut out the X, Y position of the fire attributes, because we don't believe position within the park will be a significant factor of burned area.\n",
    "2. Choose the most correlated features with respect to area.\n",
    "2. Label and One-hot Encoded the Month and Day of the fire.\n",
    "3. Standard scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghc5zmtA1Zci"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/aparande/EECS189-OMP-Assignment/main/Jupyter%20Assignment/forestfires.csv?token=ACTEYP6ONZJ5E26S3PZYJIK7ZFYMM'\n",
    "df1 = pd.read_csv(url)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hp54uH76Lqid"
   },
   "source": [
    "### Correlations\n",
    "We will now inspect the features and see if there are any we can remove. Using the correlation matrix print out the heat map of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VpyUg7EZLv7J"
   },
   "outputs": [],
   "source": [
    "# TODO: Print out a heatmap of the correlation matrix for the features\n",
    "## Use Seaborn heatmap and pandas correlation matrix functions\n",
    "# set annot = True for heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vm_GQ6f2MBIn"
   },
   "source": [
    "**Observed Features:** <br>\n",
    "From the heat map we can see that the highest correlated features by the numbers are clearly the Temperature, Relative Humidity, DMC. We will use these for our model, along with Month and Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC78313MLy6R"
   },
   "outputs": [],
   "source": [
    "df1 = df1[['month', 'day', 'temp', 'RH', 'DMC', 'area']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZqaTFAlBcIo"
   },
   "outputs": [],
   "source": [
    "# TODO: Split data into features and labels\n",
    "Xb = # Features\n",
    "yb = # Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWqpiejX9ZXA"
   },
   "outputs": [],
   "source": [
    "# One-hot encode and Standard Scale\n",
    "Xb = pd.get_dummies(Xb, columns=['month', 'day'])\n",
    "scaler = StandardScaler()\n",
    "Xb = pd.DataFrame(scaler.fit_transform(Xb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jxSxqC24DZCQ"
   },
   "source": [
    "Let's plot some of the features vs area. How many outliers can you immediately identify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPNBWMq-Do63"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot features vs area\n",
    "## plot scatter plots\n",
    "# Features: DMC, temp, Relative Humidity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9Z7xs9WD4bk"
   },
   "source": [
    "**Report your observations:** (Write your observations here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BELeJzgkdl8"
   },
   "source": [
    "## Your Turn: Perform OMP on this dataset\n",
    "Steps\n",
    "1. Split the dataset as you did before. **Use a random_state=189 for reproducability**.\n",
    "2. Fit LR model to the training data and predict on the testing data and report the MSE of the entire dataset without removing any outliers. \n",
    "3. Perform OMP on the training set and then do the same thing an perform LR on the newly outlier free dataset. (Here select the number of data points removed to be low because the number of outliers in this set does not exceed 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVrK_3AT97tO"
   },
   "outputs": [],
   "source": [
    "# TODO: split train test \n",
    "#***SOLUTION***\n",
    "X_train, X_test, y_train, y_test = # Split data\n",
    "\n",
    "# TODO: Perform Linear Regression and print the MSE\n",
    "\n",
    "ypred = #...\n",
    "MSE = #...\n",
    "Rsquared = #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JRyKusXracie"
   },
   "outputs": [],
   "source": [
    "## TODO: Implement a function to delete and return your outlier free training set\n",
    "def get_new_XY(Xnew, ynew, k):\n",
    "    # Input:\n",
    "    #   Xnew: Training Data to run outlier removal\n",
    "    #   ynew: Labels of training data to run outlier detection on\n",
    "    #   k: Sparsity (How many outliers to detect) (Recall <5)\n",
    "    # Output:\n",
    "    #   X_train_new: Outlier free feature training data\n",
    "    #   y_train_new: Outlier free label training data\n",
    "    \n",
    "    indicies = #...\n",
    "    X_train_new = #...\n",
    "    y_train_new = #...\n",
    "    return #..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_v6VNvGdarGl"
   },
   "outputs": [],
   "source": [
    "## TODO: Create A prediction function for the outlier free data\n",
    "def predict_new(X_train_new, y_train_new, xt, yt):\n",
    "    # Input:\n",
    "    #   X_train_new: Outlier free feature training data\n",
    "    #   y_train_new: Outlier free label training data\n",
    "    #   xt: Testing data to predict on\n",
    "    #   yt: Testing data labels to predict on\n",
    "    # Output:\n",
    "    #   MSE: Mean Squared Error on the Test Set\n",
    "    #   Rsquared : Correlation Coefficient on Test Set\n",
    "    \n",
    "    ## Use sklern.LinearRegression\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxkWjrcUKG0Z"
   },
   "source": [
    "Below is Code Given to you to see the effect of removing multiple outliers and the resultant MSE after a certain number of outliers are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "06EF5X4deKWt"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-aea74b541ce1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmax_outlier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcurr_X_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcurr_y_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_outlier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "mses = []\n",
    "r2 = []\n",
    "max_outlier = 100\n",
    "\n",
    "curr_X_train = X_train.to_numpy()\n",
    "curr_y_train = y_train.to_numpy()\n",
    "for k in range(0, max_outlier):\n",
    "  D = get_new_XY(curr_X_train, curr_y_train, k)\n",
    "  errors = predict_new(D[0], D[1], X_test, y_test)\n",
    "  mses.append((errors[0]))\n",
    "  r2.append(errors[1])\n",
    "ind = np.linspace(0, max_outlier-1, max_outlier)\n",
    "msesArray = np.asarray(mses)\n",
    "r2Array = np.asarray(r2)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(ind, msesArray)\n",
    "plt.xlabel(\"Number of Outliers Removed\")\n",
    "plt.ylabel(\"MSE Error on Prediction\")\n",
    "\n",
    "def MSEInteractive(k):\n",
    "  D = get_new_XY(curr_X_train, curr_y_train, k)\n",
    "  errors = predict_new(D[0], D[1], X_test, y_test)\n",
    "  mse = errors[0]\n",
    "  print(\"MSE: \", mse)\n",
    "\n",
    "interactiveMSE = interactive(MSEInteractive,\n",
    "                               k=generate_num_outliers())\n",
    "interactiveMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsiwHhodll5v"
   },
   "source": [
    "**Q: Your graph should have a steep drop and then a steady incline after. What is the approximate optimal number of outliers to remove? Why does the graph form this shape?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKa35mrqmrw_"
   },
   "source": [
    "A: (ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_PXQBIk8CbN"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKo-fBs57lXy"
   },
   "source": [
    "Remember that we used a random_state=189. Could our graph's nice shape be a fluke? Let's see if we can find situations in which our OMP outlier removal doesn't produce such a clean graph.\n",
    "\n",
    "First, let's print out the top 10 datapoints which OMP is identifying as the biggest outliers in the entire dataset (before the split), along with their respective areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9s4AM-j28E-a"
   },
   "outputs": [],
   "source": [
    "# TODO: Print outliers identified in OMP, along with their areas\n",
    "## use previously coded functions should be 2-3 lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SD7w1meNFElM"
   },
   "source": [
    "**Q: Does these agree with your observations from the scatterplot visualization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C12O5cOkFOcf"
   },
   "source": [
    "A: (ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMF79fUQ8V2h"
   },
   "source": [
    "Now, let's inspect where these outliers landed once our data was split with random_state=189."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO2Wx49n8VXe"
   },
   "outputs": [],
   "source": [
    "# TODO: Find out and print where our outliers have landed (training set or test set?)\n",
    "# Hint: you can use a try/catch block with the .loc pandas function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHvL6tdgY1KR"
   },
   "source": [
    "**Q: In the current state all, most of the top outliers are present in the training set, why might this be advantageous considering the code you have written? For example if the testing set had the top two outliers what do you think will happen?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WA5E1JyY4er"
   },
   "source": [
    "A: (ANSWER HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gfcNkgX96gf"
   },
   "source": [
    "**Try running the above cells again with different random states for the training/test split. Observe how the graph changes when the top outliers are placed in the test set and record your observations below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5KOlmEe-KSr"
   },
   "source": [
    "**Observation:** (ANSWER HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_4j4fVPUQs2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Blank Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
